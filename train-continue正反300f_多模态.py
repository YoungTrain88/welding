import glob
import os

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from sklearn.metrics import mean_absolute_error, r2_score
from torch.utils.data import DataLoader, Dataset
from torchinfo import summary
from torchvision.transforms import transforms
from tqdm import tqdm

# å¯¼å…¥è‡ªå®šä¹‰-RegressionModel
from yolo8_12_fb_all_net_MultiModal.custom_modules.custom_tasks import RegressionModel

# é…ç½®
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
# MODEL_YAML_PATH = r'C:\Users\User\Desktop\ç„Šæ¥\ultralytics-main\ultralytics-main\my_yolo_r_p1_c_s_att_conv\yoloV11n-r-att-conv.yaml'
# PRETRAINED_WEIGHTS_PATH = 'yolo11n-cls.pt'  # ä½ çš„é¢„è®­ç»ƒæƒé‡
PROJECT_NAME = "yolo8_12_f_all_net_MultiModal"
EPOCHS = 100  # è°ƒæ•´è®­ç»ƒè½®æ•°çœ‹R2èƒ½å¦æœ‰æ‰€æé«˜
TRAIN_CSV_PATH = os.path.join(PROJECT_ROOT, PROJECT_NAME, "datasets", "train.csv")
VAL_CSV_PATH = os.path.join(PROJECT_ROOT, PROJECT_NAME, "datasets", "val.csv")
# SAVE_DIR = os.path.join('my_yolo_r_p1_c_s_att_conv', 'runs-yolo11n-AFAR')  # å¯è‡ªå®šä¹‰

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

BATCH_SIZE = 8
LEARNING_RATE = 1e-5  # æŠŠå­¦ä¹ ç‡å†è°ƒä½åˆ°0.00001ï¼Œçœ‹è®­ç»ƒæ›²çº¿æ˜¯å¦è¿˜ä¼šä¸€ç›´éœ‡è¡

IMG_SIZE = 224

# os.makedirs(SAVE_DIR, exist_ok=True)

# class RegressionDataset(Dataset):
#     def __init__(self, csv_path, transform=None):
#         self.data_frame = pd.read_csv(csv_path)
#         self.transform = transform

#     def __len__(self):
#         return len(self.data_frame)

#     def __getitem__(self, idx):
#         img_relative_path = self.data_frame.iloc[idx, 0]
#         img_abs_path = os.path.join(PROJECT_ROOT, PROJECT_NAME, img_relative_path)
#         image = Image.open(img_abs_path).convert("RGB")
#         value = torch.tensor([self.data_frame.iloc[idx, 1]], dtype=torch.float32)
#         if self.transform:
#             image = self.transform(image)
#         return image, value


class RegressionDataset(Dataset):
    def __init__(self, csv_path, transform=None):
        self.data_frame = pd.read_csv(csv_path)
        self.transform = transform
        self.tabular_data = self.data_frame.iloc[:, 2:].values.astype("float32")

    def __len__(self):
        return len(self.data_frame)

    def __getitem__(self, idx):
        img_relative_path = self.data_frame.iloc[idx, 0]
        img_abs_path = os.path.join(PROJECT_ROOT, PROJECT_NAME, img_relative_path)
        image = Image.open(img_abs_path).convert("RGB")
        if self.transform:
            image = self.transform(image)

        tabular_features = torch.tensor(self.tabular_data[idx], dtype=torch.float32)
        target_value = torch.tensor([self.data_frame.iloc[idx, 1]], dtype=torch.float32)

        return image, tabular_features, target_value


def train_one_yaml(yaml_path):
    print(f"\n========== å¼€å§‹è®­ç»ƒ: {yaml_path} ==========")
    # ä¿å­˜è·¯å¾„æ”¹ä¸ºyolo8-12-æ­£é¢/runs-xxx
    yaml_name = os.path.splitext(os.path.basename(yaml_path))[0]
    save_dir = os.path.join(PROJECT_ROOT, PROJECT_NAME, f"runs-{yaml_name}")
    os.makedirs(save_dir, exist_ok=True)

    # å…¶ä½™é…ç½®å’Œæ•°æ®åŠ è½½ä¸å˜
    # transform = transforms.Compose([
    #     transforms.Resize((IMG_SIZE, IMG_SIZE)),
    #     transforms.ToTensor(),
    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    # ])

    train_transform = transforms.Compose(
        [
            transforms.Resize((IMG_SIZE, IMG_SIZE)),
            transforms.RandomHorizontalFlip(p=0.5),  # <--- æ–°å¢ï¼šéšæœºæ°´å¹³ç¿»è½¬
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # <--- æ–°å¢ï¼šéšæœºé¢œè‰²æŠ–åŠ¨
            transforms.RandomRotation(10),  # <--- æ–°å¢ï¼šéšæœºæ—‹è½¬ +/-10åº¦
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    val_transform = transforms.Compose(
        [  # éªŒè¯é›†çš„å˜æ¢ä¿æŒä¸å˜
            transforms.Resize((IMG_SIZE, IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )
    train_dataset = RegressionDataset(csv_path=TRAIN_CSV_PATH, transform=train_transform)
    val_dataset = RegressionDataset(csv_path=VAL_CSV_PATH, transform=val_transform)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    # ç¡®å®šä½ çš„è¡¨æ ¼æ•°æ®æœ‰å¤šå°‘ä¸ªç‰¹å¾
    # ä¾‹å¦‚ï¼Œä»CSVæ–‡ä»¶ä¸­åŠ¨æ€è·å–
    temp_df = pd.read_csv(TRAIN_CSV_PATH)
    NUM_TABULAR_FEATURES = len(temp_df.columns) - 2  # å‡å» path å’Œ target
    print(f"æ£€æµ‹åˆ° {NUM_TABULAR_FEATURES} ä¸ªè¡¨æ ¼ç‰¹å¾ã€‚")

    # åˆå§‹åŒ–æ–°çš„å¤šæ¨¡æ€æ¨¡å‹
    net = MultiModalModel(yaml_path=yaml_path, num_tabular_features=NUM_TABULAR_FEATURES).to(DEVICE)
    # è·å–è¡¨æ ¼ç‰¹å¾æ•°é‡
    temp_df = pd.read_csv(TRAIN_CSV_PATH)
    NUM_TABULAR_FEATURES = len(temp_df.columns) - 2
    print(f"æ£€æµ‹åˆ° {NUM_TABULAR_FEATURES} ä¸ªè¡¨æ ¼ç‰¹å¾ã€‚")

    # åˆå§‹åŒ–æ–°çš„å¤šæ¨¡æ€æ¨¡å‹
    net = MultiModalModel(yaml_path=yaml_path, num_tabular_features=NUM_TABULAR_FEATURES).to(DEVICE)

    # ==================== åœ¨è¿™é‡Œæ·»åŠ æ¨¡å‹ç»“æ„æ‰“å°ä»£ç  ====================
    print("\n" + "=" * 50)
    print(f"             æ¨¡å‹ç»“æ„: {os.path.basename(yaml_path)}")
    print("=" * 50)

    # å®šä¹‰æ¨¡å‹çš„è¾“å…¥å°ºå¯¸ï¼Œæ³¨æ„æˆ‘ä»¬æœ‰ä¸¤ä¸ªè¾“å…¥ï¼Œæ‰€ä»¥æä¾›ä¸€ä¸ªåˆ—è¡¨
    # (batch_size, channels, height, width) for image
    # (batch_size, num_features) for tabular data
    # IMG_SIZE æ˜¯æ‚¨åœ¨è„šæœ¬é…ç½®ä¸­å®šä¹‰çš„å›¾åƒå°ºå¯¸
    input_sizes = [(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE), (BATCH_SIZE, NUM_TABULAR_FEATURES)]

    # ä½¿ç”¨ torchinfo æ‰“å°è¯¦ç»†æ‘˜è¦
    # col_names æŒ‡å®šäº†è¦æ˜¾ç¤ºçš„åˆ—
    summary(
        net, input_size=input_sizes, col_names=["input_size", "output_size", "num_params", "mult_adds"], depth=3
    )  # depth æ§åˆ¶æ˜¾ç¤ºå­æ¨¡å—çš„æ·±åº¦

    print("=" * 50 + "\n")
    # ========================== æ‰“å°ä»£ç ç»“æŸ ==========================

    criterion = nn.MSELoss()
    # ä¿®æ”¹è¿™é‡Œï¼ŒåŠ å…¥ weight_decay
    optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)  # <--- å¢åŠ  weight_decay
    # ... (scheduler å’Œå…¶ä»–å˜é‡å®šä¹‰ä¸å˜) ...
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    best_val_loss = float("inf")
    train_losses, val_losses, val_maes, val_r2s = [], [], [], []

    for epoch in range(EPOCHS):
        net.train()
        running_loss = 0.0
        train_pbar = tqdm(train_loader, desc=f"[{yaml_name}] Epoch {epoch + 1}/{EPOCHS} [è®­ç»ƒ]")
        # ä¿®æ”¹æ•°æ®åŠ è½½å¾ªç¯
        for images, tabular, labels in train_pbar:
            images, tabular, labels = images.to(DEVICE), tabular.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()

            # å°†å›¾åƒå’Œè¡¨æ ¼æ•°æ®åŒæ—¶ä¼ å…¥æ¨¡å‹
            outputs = net(images, tabular)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            # ... (æ›´æ–° loss å’Œè¿›åº¦æ¡çš„é€»è¾‘ä¸å˜) ...
            running_loss += loss.item()
            train_pbar.set_postfix({"loss": loss.item()})
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        net.eval()
        val_loss, all_preds, all_labels = 0.0, [], []
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f"[{yaml_name}] Epoch {epoch + 1}/{EPOCHS} [éªŒè¯]")
            # ä¿®æ”¹éªŒè¯å¾ªç¯
            for images, tabular, labels in val_pbar:
                images, tabular, labels = images.to(DEVICE), tabular.to(DEVICE), labels.to(DEVICE)

                # å°†å›¾åƒå’Œè¡¨æ ¼æ•°æ®åŒæ—¶ä¼ å…¥æ¨¡å‹
                outputs = net(images, tabular)

                loss = criterion(outputs, labels)
                val_loss += loss.item()
                val_pbar.set_postfix({"val_loss": loss.item()})
                all_preds.extend(outputs.cpu().numpy().flatten())
                all_labels.extend(labels.cpu().numpy().flatten())
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        val_mae = mean_absolute_error(all_labels, all_preds)
        val_r2 = r2_score(all_labels, all_preds)
        val_maes.append(val_mae)
        val_r2s.append(val_r2)

        print(
            f"[{yaml_name}] Epoch {epoch + 1}/{EPOCHS} -> è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_val_loss:.4f} | éªŒè¯MAE: {val_mae:.4f} | éªŒè¯R2: {val_r2:.4f}"
        )
        scheduler.step()
        torch.save(net.state_dict(), os.path.join(save_dir, "last.pt"))
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(net.state_dict(), os.path.join(save_dir, "best.pt"))
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")

    # ä¿å­˜lossç­‰
    with open(os.path.join(save_dir, "train_losses.txt"), "w", encoding="utf-8") as f:
        for loss in train_losses:
            f.write(f"{loss}\n")
    with open(os.path.join(save_dir, "val_losses.txt"), "w", encoding="utf-8") as f:
        for loss in val_losses:
            f.write(f"{loss}\n")
    with open(os.path.join(save_dir, "val_maes.txt"), "w", encoding="utf-8") as f:
        for mae in val_maes:
            f.write(f"{mae}\n")
    with open(os.path.join(save_dir, "val_r2s.txt"), "w", encoding="utf-8") as f:
        for r2 in val_r2s:
            f.write(f"{r2}\n")


class MultiModalModel(nn.Module):
    def __init__(self, yaml_path, num_tabular_features, hidden_dim=64):
        """
        Args:
            yaml_path (str): å›¾åƒæ¨¡å‹çš„yamlé…ç½®æ–‡ä»¶è·¯å¾„.
            num_tabular_features (int): è¾“å…¥çš„è¡¨æ ¼/æ•°å€¼ç‰¹å¾çš„æ•°é‡.
            hidden_dim (int): MLPå’Œèåˆå¤´çš„éšè—å±‚ç»´åº¦.
        """
        super().__init__()

        # 1. å›¾åƒåˆ†æ”¯: åŠ è½½æˆ‘ä»¬ä¿®æ”¹è¿‡çš„RegressionModel
        # æˆ‘ä»¬å°†ä»¥â€œç‰¹å¾æå–æ¨¡å¼â€æ¥ä½¿ç”¨å®ƒ
        self.image_branch = RegressionModel(yaml_path, ch=3, verbose=False)  # verbose=Falseé¿å…é‡å¤æ‰“å°æ¨¡å‹ç»“æ„

        # ä»RegressionHeadä¸­æˆ‘ä»¬çŸ¥é“å›¾åƒç‰¹å¾ç»´åº¦æ˜¯1280
        img_feature_dim = 1280

        # 2. è¡¨æ ¼æ•°æ®åˆ†æ”¯ (ä¸€ä¸ªç®€å•çš„MLP)
        self.tabular_branch = nn.Sequential(
            nn.Linear(num_tabular_features, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.5),  # <--- ä» 0.2 å¢åŠ åˆ° 0.5
            nn.Linear(hidden_dim * 2, hidden_dim),
        )

        # 3. èåˆåçš„å›å½’å¤´
        self.fusion_head = nn.Sequential(
            nn.Linear(img_feature_dim + hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),  # <--- ä» 0.2 å¢åŠ åˆ° 0.5
            nn.Linear(hidden_dim, 1),  # æœ€åè¾“å‡ºä¸€ä¸ªå›å½’å€¼
        )

    def forward(self, image, tabular_data):
        # 1. ä»å›¾åƒåˆ†æ”¯è·å–å›¾åƒç‰¹å¾å‘é‡ (1280ç»´)
        # è°ƒç”¨æ—¶ä¼ å…¥ return_features=True
        image_features = self.image_branch(image, return_features=True)

        # 2. ä»è¡¨æ ¼æ•°æ®åˆ†æ”¯è·å–ç‰¹å¾
        tabular_features = self.tabular_branch(tabular_data)

        # 3. èåˆç‰¹å¾ (æ‹¼æ¥)
        combined_features = torch.cat([image_features, tabular_features], dim=1)

        # 4. é€šè¿‡å›å½’å¤´å¾—åˆ°æœ€ç»ˆç»“æœ
        output = self.fusion_head(combined_features)
        return output


if __name__ == "__main__":
    yaml_dir = os.path.join(PROJECT_ROOT, PROJECT_NAME, "yaml")
    yaml_files = glob.glob(os.path.join(yaml_dir, "*.yaml"))
    print(f"å…±æ£€æµ‹åˆ° {len(yaml_files)} ä¸ªyamlæ–‡ä»¶ï¼Œå°†ä¾æ¬¡è®­ç»ƒï¼š")
    for yaml_path in yaml_files:
        train_one_yaml(yaml_path)
