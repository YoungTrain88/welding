# train_resnet.py

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import transforms
import torchvision.models as models
from PIL import Image
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import mean_absolute_error, r2_score

# ===================================================================
# 1. å®šä¹‰è¶…å‚æ•°å’Œé…ç½®
# ===================================================================

# è·¯å¾„é…ç½® (è„šæœ¬ä¼šè‡ªåŠ¨è·å–å½“å‰ç›®å½•ä½œä¸ºé¡¹ç›®æ ¹ç›®å½•)'
PROJECT_ROOT = r'C:\Users\User\Desktop\ç„Šæ¥\ultralytics-main\ultralytics-main\my_yolo_regression_project1-cat-shuffed'
# å‡è®¾æ‚¨çš„æ•°æ®å­˜å‚¨åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ 'datasets' æ–‡ä»¶å¤¹ä¸­
TRAIN_CSV_PATH = os.path.join(PROJECT_ROOT, 'datasets', 'train.csv')
VAL_CSV_PATH = os.path.join(PROJECT_ROOT, 'datasets', 'val.csv')
SAVE_DIR = os.path.join(PROJECT_ROOT, 'runs_resnet50') # ä¸ºResNetåˆ›å»ºä¸€ä¸ªæ–°çš„ä¿å­˜ç›®å½•

# è®­ç»ƒé…ç½®
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
EPOCHS = 300
BATCH_SIZE = 16 # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒä½æ­¤å€¼ï¼Œä¾‹å¦‚ 8
LEARNING_RATE = 1e-4
IMG_SIZE = 224 # ResNeté€šå¸¸ä½¿ç”¨ 224x224 æˆ– 256x256 çš„è¾“å…¥å°ºå¯¸

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs(SAVE_DIR, exist_ok=True)


# ===================================================================
# 2. ResNet50 å›å½’æ¨¡å‹åˆ›å»ºå‡½æ•°
# ===================================================================
def create_resnet50_regression():
    """
    åŠ è½½é¢„è®­ç»ƒçš„ ResNet50 å¹¶å°†å…¶æœ«å±‚ä¿®æ”¹ä¸ºå›å½’å¤´ã€‚
    """
    # 1. åŠ è½½é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹
    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)

    # 2. è·å–åŸå§‹åˆ†ç±»å¤´çš„è¾“å…¥ç‰¹å¾æ•°
    # åœ¨ ResNet ä¸­ï¼Œåˆ†ç±»å¤´åä¸º 'fc' (fully-connected)
    num_ftrs = model.fc.in_features

    # 3. ç”¨ä¸€ä¸ªæ–°çš„çº¿æ€§å±‚æ›¿æ¢æ‰åŸæ¥çš„åˆ†ç±»å¤´
    # æ–°çš„çº¿æ€§å±‚è¾“å‡ºç»´åº¦ä¸º1ï¼Œç”¨äºå›å½’
    model.fc = nn.Linear(num_ftrs, 1)
    
    return model


# ===================================================================
# 3. è‡ªå®šä¹‰æ•°æ®é›†ç±» (ä¸ä¹‹å‰å®Œå…¨ç›¸åŒ)
# ===================================================================
class RegressionDataset(Dataset):
    def __init__(self, csv_path, transform=None):
        self.data_frame = pd.read_csv(csv_path)
        self.transform = transform

    def __len__(self):
        return len(self.data_frame)

    def __getitem__(self, idx):
        # è¿™é‡Œçš„è·¯å¾„æ˜¯ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„ï¼Œä¾‹å¦‚ 'datasets/images/001.jpg'
        img_relative_path = self.data_frame.iloc[idx, 0]
        img_abs_path = os.path.join(PROJECT_ROOT, img_relative_path)
        
        try:
            image = Image.open(img_abs_path).convert("RGB")
        except FileNotFoundError:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡ {img_abs_path}")
            # è¿”å›ä¸€ä¸ªå‡çš„ç©ºæ•°æ®ï¼Œæˆ–è€…æ‚¨å¯ä»¥é€‰æ‹©æŠ›å‡ºå¼‚å¸¸
            return torch.zeros(3, IMG_SIZE, IMG_SIZE), torch.tensor([0.0])

        value = torch.tensor([self.data_frame.iloc[idx, 1]], dtype=torch.float32)

        if self.transform:
            image = self.transform(image)
        return image, value


# ===================================================================
# 4. ä¸»è®­ç»ƒé€»è¾‘
# ===================================================================
def main():
    print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    print("-" * 30)

    # --- æ•°æ®åŠ è½½ ---
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    train_dataset = RegressionDataset(csv_path=TRAIN_CSV_PATH, transform=transform)
    val_dataset = RegressionDataset(csv_path=VAL_CSV_PATH, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
    
    print("æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæ¯•ã€‚")
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}, éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    print("-" * 30)

    # --- æ¨¡å‹åˆå§‹åŒ– ---
    print("æ­£åœ¨åˆ›å»º ResNet50 å›å½’æ¨¡å‹...")
    net = create_resnet50_regression().to(DEVICE)
    print("æ¨¡å‹åŠ è½½å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ã€‚")
    print("-" * 30)

    # --- æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    criterion = nn.MSELoss() # å‡æ–¹è¯¯å·®æŸå¤±
    optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    # --- è®­ç»ƒå¾ªç¯ ---
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    val_maes = []
    val_r2s = []

    for epoch in range(EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        net.train()
        running_loss = 0.0
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [è®­ç»ƒä¸­]")
        for images, labels in train_pbar:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = net(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # éªŒè¯é˜¶æ®µ
        net.eval()
        val_loss = 0.0
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                outputs = net(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                all_preds.extend(outputs.cpu().numpy().flatten())
                all_labels.extend(labels.cpu().numpy().flatten())
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        # è®¡ç®—å›å½’æŒ‡æ ‡
        val_mae = mean_absolute_error(all_labels, all_preds)
        val_r2 = r2_score(all_labels, all_preds)
        val_maes.append(val_mae)
        val_r2s.append(val_r2)

        print(f"Epoch {epoch+1}/{EPOCHS} -> è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_val_loss:.4f} | éªŒè¯MAE: {val_mae:.4f} | éªŒè¯R2: {val_r2:.4f}")

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # --- ä¿å­˜æ¨¡å‹ ---
        torch.save(net.state_dict(), os.path.join(SAVE_DIR, 'last.pt'))
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(net.state_dict(), os.path.join(SAVE_DIR, 'best.pt'))
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")

    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {os.path.join(SAVE_DIR, 'best.pt')}")

    # ä¿å­˜æŸå¤±å’ŒæŒ‡æ ‡
    with open(os.path.join(SAVE_DIR, 'train_losses.txt'), 'w', encoding='utf-8') as f:
        for loss in train_losses:
            f.write(f"{loss}\n")
    with open(os.path.join(SAVE_DIR, 'val_losses.txt'), 'w', encoding='utf-8') as f:
        for loss in val_losses:
            f.write(f"{loss}\n")
    with open(os.path.join(SAVE_DIR, 'val_maes.txt'), 'w', encoding='utf-8') as f:
        for mae in val_maes:
            f.write(f"{mae}\n")
    with open(os.path.join(SAVE_DIR, 'val_r2s.txt'), 'w', encoding='utf-8') as f:
        for r2 in val_r2s:
            f.write(f"{r2}\n")


if __name__ == '__main__':
    # ç¡®ä¿æ‚¨çš„CSVæ–‡ä»¶å’Œå›¾ç‰‡è·¯å¾„æ­£ç¡®
    if not os.path.exists(TRAIN_CSV_PATH) or not os.path.exists(VAL_CSV_PATH):
        print("="*50)
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è®­ç»ƒæˆ–éªŒè¯CSVæ–‡ä»¶ã€‚")
        print(f"è¯·ç¡®ä¿ '{TRAIN_CSV_PATH}' å’Œ '{VAL_CSV_PATH}' æ–‡ä»¶å­˜åœ¨ã€‚")
        print("="*50)
    else:
        main()