# run_experiments.py

import os

import numpy as np
import pandas as pd
import timm  # å¯¼å…¥timmåº“
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from PIL import Image
from sklearn.metrics import mean_absolute_error, r2_score
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import transforms
from tqdm import tqdm

# ===================================================================
# 1. å…¨å±€é…ç½® (åŸºæœ¬ä¸å˜)
# ===================================================================

PROJECT_ROOT = r"C:\Users\User\Desktop\ç„Šæ¥\ultralytics-main\ultralytics-main\å…¶ä»–æ¨¡å‹-æ­£åæ‹¼æ¥300è½®"
EPOCHS = 300  # å»ºè®®å…ˆç”¨è¾ƒå°çš„epochï¼ˆå¦‚50ï¼‰å¿«é€Ÿè¿­ä»£ï¼Œæ‰¾åˆ°å¥½æ¨¡å‹åå†å¢åŠ 
TRAIN_CSV_PATH = os.path.join(PROJECT_ROOT, "datasets", "train.csv")
VAL_CSV_PATH = os.path.join(PROJECT_ROOT, "datasets", "val.csv")

# è®­ç»ƒè¶…å‚æ•°
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

BATCH_SIZE = 16
LEARNING_RATE = 1e-4
IMG_SIZE = 224


# ===================================================================
# 2. æ¨¡å‹å·¥å‚ï¼šæ ¹æ®åç§°åˆ›å»ºä¸åŒçš„å›å½’æ¨¡å‹
# ===================================================================
def create_regression_model(model_name: str):
    """
    æ ¹æ®æ¨¡å‹åç§°ï¼Œåˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªä¿®æ”¹å¥½ç”¨äºå›å½’çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚.

    Args:
        model_name (str): æ¨¡å‹çš„åç§°, e.g., 'resnet50', 'efficientnet_b0'.

    Returns:
        torch.nn.Module: ä¸€ä¸ªå‡†å¤‡å¥½è¿›è¡Œå›å½’è®­ç»ƒçš„æ¨¡å‹ã€‚
    """
    print(f"--- æ­£åœ¨åˆ›å»ºæ¨¡å‹: {model_name} ---")

    # æ–¹æ¡ˆAï¼šä½¿ç”¨ torchvision åŠ è½½ç»å…¸æ¨¡å‹
    if model_name == "resnet50":
        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, 1)

    elif model_name == "vgg16":
        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
        num_ftrs = model.classifier[6].in_features
        model.classifier[6] = nn.Linear(num_ftrs, 1)

    elif model_name == "densenet121":
        model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)
        num_ftrs = model.classifier.in_features
        model.classifier = nn.Linear(num_ftrs, 1)

    elif model_name == "mobilenet_v3_large":
        model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2)
        num_ftrs = model.classifier[3].in_features
        model.classifier[3] = nn.Linear(num_ftrs, 1)

    # æ–¹æ¡ˆBï¼šä½¿ç”¨ timm åŠ è½½æ›´å¤šã€æ›´ç°ä»£çš„æ¨¡å‹ (æ›´æ¨è)
    # timmçš„create_modelæ¥å£éå¸¸ç»Ÿä¸€ï¼Œå¯ä»¥ç›´æ¥ç”¨num_classes=1æ¥åˆ›å»ºå›å½’å¤´
    elif "efficientnet" in model_name:
        model = timm.create_model(model_name, pretrained=True, num_classes=1)

    elif "vit" in model_name:  # Vision Transformer
        model = timm.create_model(model_name, pretrained=True, num_classes=1)

    elif "resnext" in model_name:
        model = timm.create_model(model_name, pretrained=True, num_classes=1)

    elif "swin" in model_name:  # Swin Transformer
        model = timm.create_model(model_name, pretrained=True, num_classes=1)

    elif "convnext" in model_name:  # ConvNeXt
        model = timm.create_model(model_name, pretrained=True, num_classes=1)

    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹åç§°: {model_name}")

    return model


# ===================================================================
# 3. æ•°æ®é›†ç±» (ä¿®å¤æ–‡ä»¶è¯»å–é—®é¢˜)
# ===================================================================
class RegressionDataset(Dataset):
    def __init__(self, csv_path, transform=None):
        self.data_frame = pd.read_csv(csv_path)
        self.transform = transform
        self.missing_files = 0  # ç»Ÿè®¡ç¼ºå¤±æ–‡ä»¶æ•°é‡

    def __len__(self):
        return len(self.data_frame)

    def __getitem__(self, idx):
        img_relative_path = self.data_frame.iloc[idx, 0]
        img_abs_path = os.path.join(PROJECT_ROOT, img_relative_path)

        # è·å–ç›®æ ‡å€¼
        target_value = float(self.data_frame.iloc[idx, 1])

        try:
            image = Image.open(img_abs_path).convert("RGB")
        except FileNotFoundError:
            print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {img_abs_path}")
            self.missing_files += 1
            # è¿”å›é»‘è‰²å›¾åƒï¼Œä½†ä¿æŒæ­£ç¡®çš„ç›®æ ‡å€¼
            image = Image.new("RGB", (IMG_SIZE, IMG_SIZE), color="black")

        # ç¡®ä¿ç›®æ ‡å€¼æ­£ç¡®ä¼ é€’
        value = torch.tensor([target_value], dtype=torch.float32)

        if self.transform:
            image = self.transform(image)

        return image, value


# ===================================================================
# 4. é€šç”¨è®­ç»ƒå‡½æ•° (ä»mainå‡½æ•°é‡æ„è€Œæ¥)
# ===================================================================
def run_training_session(model, model_name, train_loader, val_loader, save_dir):
    """å¯¹ç»™å®šçš„æ¨¡å‹æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå’ŒéªŒè¯æµç¨‹ã€‚."""
    print(f"æ¨¡å‹å·²ç§»è‡³è®¾å¤‡: {DEVICE}")
    model.to(DEVICE)

    # --- æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    # --- è®­ç»ƒå¾ªç¯ ---
    best_val_loss = float("inf")
    train_losses = []
    val_losses = []
    val_maes = []
    val_r2s = []

    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        train_pbar = tqdm(train_loader, desc=f"æ¨¡å‹: {model_name} | Epoch {epoch + 1}/{EPOCHS} [è®­ç»ƒ]")
        for images, labels in train_pbar:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            train_pbar.set_postfix({"loss": f"{loss.item():.4f}"})
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss, all_preds, all_labels = 0.0, [], []
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                all_preds.extend(outputs.cpu().numpy().flatten())
                all_labels.extend(labels.cpu().numpy().flatten())
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        val_mae = mean_absolute_error(all_labels, all_preds)

        # ä¿®å¤R2è®¡ç®—çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜
        try:
            val_r2 = r2_score(all_labels, all_preds)
            # å¦‚æœR2æ˜¯NaNæˆ–æ— ç©·å¤§ï¼Œè®¾ç½®ä¸º0
            if not np.isfinite(val_r2):
                val_r2 = 0.0
        except:
            val_r2 = 0.0

        val_maes.append(val_mae)
        val_r2s.append(val_r2)

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        pred_range = f"[{min(all_preds):.4f}, {max(all_preds):.4f}]"
        label_range = f"[{min(all_labels):.4f}, {max(all_labels):.4f}]"
        print(f"è°ƒè¯•ä¿¡æ¯ -> é¢„æµ‹å€¼èŒƒå›´: {pred_range} | çœŸå®å€¼èŒƒå›´: {label_range}")
        print(
            f"ç»“æœ -> è®­ç»ƒæŸå¤±: {avg_train_loss:.6f} | éªŒè¯æŸå¤±: {avg_val_loss:.6f} | éªŒè¯MAE: {val_mae:.6f} | éªŒè¯R2: {val_r2:.6f}"
        )
        scheduler.step()

        # --- ä¿å­˜æ¨¡å‹ ---
        torch.save(model.state_dict(), os.path.join(save_dir, "last.pt"))
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), os.path.join(save_dir, "best.pt"))
            print(f"ğŸ‰ æ¨¡å‹ {model_name} å‘ç°æ–°çš„æœ€ä½³æƒé‡, éªŒè¯æŸå¤±: {best_val_loss:.4f}")

    print(f"\næ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'best.pt')}")

    # ä¿å­˜æ¯ä¸€è½®çš„æŸå¤±å’ŒæŒ‡æ ‡
    with open(os.path.join(save_dir, "train_losses.txt"), "w", encoding="utf-8") as f:
        for loss in train_losses:
            f.write(f"{loss}\n")
    with open(os.path.join(save_dir, "val_losses.txt"), "w", encoding="utf-8") as f:
        for loss in val_losses:
            f.write(f"{loss}\n")
    with open(os.path.join(save_dir, "val_maes.txt"), "w", encoding="utf-8") as f:
        for mae in val_maes:
            f.write(f"{mae}\n")
    with open(os.path.join(save_dir, "val_r2s.txt"), "w", encoding="utf-8") as f:
        for r2 in val_r2s:
            f.write(f"{r2}\n")


# ===================================================================
# 5. ä¸»æ‰§è¡Œå™¨
# ===================================================================
if __name__ == "__main__":
    # ğŸš€ åœ¨è¿™é‡Œå®šä¹‰æ‚¨æƒ³è¿›è¡Œå¯¹æ¯”å®éªŒçš„æ‰€æœ‰æ¨¡å‹ï¼
    # æ‚¨å¯ä»¥æ³¨é‡Šæ‰ä¸æƒ³è·‘çš„æ¨¡å‹ï¼Œæˆ–è€…æ·»åŠ timmåº“æ”¯æŒçš„å…¶ä»–æ¨¡å‹åã€‚
    models_to_train = [
        # --- ç»å…¸æ¨¡å‹ (æ¥è‡ª torchvision) ---
        # 'resnet50',        # ç»å…¸åŸºå‡†
        # 'densenet121',     # å¯†é›†è¿æ¥ï¼Œå‚æ•°é«˜æ•ˆ
        # 'mobilenet_v3_large', # ä¼˜ç§€çš„è½»é‡çº§æ¨¡å‹
        "vgg16",  # ç»“æ„ç®€å•ï¼Œæ·±åº¦å­¦ä¹ çš„æ—©æœŸé‡Œç¨‹ç¢‘
        # --- ç°ä»£SOTAæ¨¡å‹ (æ¥è‡ª timm) ---
        "efficientnet_b0",  # æ€§èƒ½ä¸æ•ˆç‡çš„å®Œç¾å¹³è¡¡
        "resnext50_32x4d",  # ResNetçš„å¼ºå¤§æ”¹è¿›ç‰ˆ
        "convnext_tiny",  # ç°ä»£åŒ–çš„çº¯å·ç§¯æ¨¡å‹
        # --- Transformeræ¶æ„ ---
        "vit_base_patch16_224",  # Vision Transformer åŸºç¡€ç‰ˆ
        "swin_tiny_patch4_window7_224",  # Swin Transformer, å¯¹ViTçš„æ”¹è¿›
        # æ‚¨è¿˜å¯ä»¥æ·»åŠ æ›´å¤štimmæ”¯æŒçš„æ¨¡å‹...
        # 'efficientnet_b2',
        # 'regnetx_002',
    ]

    # --- æ•°æ®åŠ è½½ (åªéœ€è¦æ‰§è¡Œä¸€æ¬¡) ---
    transform = transforms.Compose(
        [
            transforms.Resize((IMG_SIZE, IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )
    train_dataset = RegressionDataset(csv_path=TRAIN_CSV_PATH, transform=transform)
    val_dataset = RegressionDataset(csv_path=VAL_CSV_PATH, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

    # æ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    print("æ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
    train_targets = [train_dataset.data_frame.iloc[i, 1] for i in range(len(train_dataset))]
    val_targets = [val_dataset.data_frame.iloc[i, 1] for i in range(len(val_dataset))]

    print(f"è®­ç»ƒé›†ç›®æ ‡å€¼èŒƒå›´: {min(train_targets):.6f} - {max(train_targets):.6f}")
    print(f"éªŒè¯é›†ç›®æ ‡å€¼èŒƒå›´: {min(val_targets):.6f} - {max(val_targets):.6f}")
    print(f"è®­ç»ƒé›†ç›®æ ‡å€¼å‡å€¼: {np.mean(train_targets):.6f}")
    print(f"éªŒè¯é›†ç›®æ ‡å€¼å‡å€¼: {np.mean(val_targets):.6f}")

    # æµ‹è¯•æ•°æ®åŠ è½½å™¨çš„å®é™…è¾“å‡º
    print("\néªŒè¯æ•°æ®åŠ è½½å™¨è¾“å‡º:")
    test_batch = next(iter(val_loader))
    test_images, test_labels = test_batch
    print(f"æµ‹è¯•æ‰¹æ¬¡æ ‡ç­¾èŒƒå›´: {test_labels.min().item():.6f} - {test_labels.max().item():.6f}")
    print(f"æµ‹è¯•æ‰¹æ¬¡æ ‡ç­¾å½¢çŠ¶: {test_labels.shape}")

    # æ£€æŸ¥ç¼ºå¤±æ–‡ä»¶
    if hasattr(train_dataset, "missing_files") and train_dataset.missing_files > 0:
        print(f"âš ï¸  è®­ç»ƒé›†ç¼ºå¤±æ–‡ä»¶æ•°é‡: {train_dataset.missing_files}")
    if hasattr(val_dataset, "missing_files") and val_dataset.missing_files > 0:
        print(f"âš ï¸  éªŒè¯é›†ç¼ºå¤±æ–‡ä»¶æ•°é‡: {val_dataset.missing_files}")
    print("æ•°æ®åŠ è½½å™¨å·²åˆ›å»ºï¼Œå‡†å¤‡å¼€å§‹å®éªŒ...")

    # --- å¾ªç¯æ‰§è¡Œæ‰€æœ‰å®éªŒ ---
    for model_name in models_to_train:
        print(f"\n{'=' * 25} æ­£åœ¨å¼€å§‹æ¨¡å‹: {model_name.upper()} çš„å®éªŒ {'=' * 25}")

        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç‹¬ç«‹çš„ä¿å­˜ç›®å½•
        current_save_dir = os.path.join(PROJECT_ROOT, f"runs_{model_name}")
        os.makedirs(current_save_dir, exist_ok=True)

        # åˆ›å»ºæ¨¡å‹
        model = create_regression_model(model_name)

        # æ‰§è¡Œè®­ç»ƒ
        run_training_session(model, model_name, train_loader, val_loader, current_save_dir)

        # (å¯é€‰) æ¸…ç†æ˜¾å­˜
        del model
        torch.cuda.empty_cache()

    print("\næ‰€æœ‰å®éªŒå·²å®Œæˆï¼")
